{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ddeef8-a63f-473a-87b1-25016dbac664",
   "metadata": {},
   "source": [
    "# CMM307 - Advanced Artificial Intelligence\n",
    "### ***(Valentin Kiss - 1608118)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "271d0085-9ffc-4cae-b951-6ad99b25ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries used throughout the notebook\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, GlobalMaxPooling1D, Dense, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from lbl2vec import Lbl2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e434141-24df-4504-9434-e7ff70aab740",
   "metadata": {},
   "source": [
    "## Section 1 - Dataset\n",
    "\n",
    "(Identify the problem and why an NLP solution is needed. Make reference to the dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c25b30-5eb0-4f85-99b9-c78d55ec8fc6",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "***Load and Explore the dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a19a02-305f-411c-b606-c0009637a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train & test datasets into a Pandas DF\n",
    "train_df = pd.read_csv('./ag_news_data/train.csv')\n",
    "test_df = pd.read_csv('./ag_news_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd3344b7-28d4-4b78-9af9-6373fdecc688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class Index                                              Title  \\\n",
       "0            3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1            3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2            3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3            3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4            3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         Description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the first few rows in the dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48923a8-dc3d-4e9f-b033-0320dc9b4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "train_df.columns = ['class', 'title', 'description']\n",
    "test_df.columns = ['class', 'title', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97efbe7e-04e3-4507-add9-c887d65326dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (120000, 3)\n",
      "Testing data shape: (7600, 3)\n"
     ]
    }
   ],
   "source": [
    "# Output the shape of both the train and test DFs\n",
    "print(f'Training data shape: {train_df.shape}')\n",
    "print(f'Testing data shape: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e665ed62-1f37-4fbb-b67d-33addbc893db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "3    30000\n",
      "4    30000\n",
      "2    30000\n",
      "1    30000\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "3    1900\n",
      "4    1900\n",
      "2    1900\n",
      "1    1900\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Output class distribution for train / test data\n",
    "print(train_df['class'].value_counts())\n",
    "print(test_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d5a7c4-d175-45ab-931a-6657e7a7a6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    120000.000000\n",
      "mean        193.388517\n",
      "std          64.472066\n",
      "min           6.000000\n",
      "25%         155.000000\n",
      "50%         188.000000\n",
      "75%         219.000000\n",
      "max         985.000000\n",
      "Name: description, dtype: float64\n",
      "count    7600.000000\n",
      "mean      192.408026\n",
      "std        63.192774\n",
      "min        37.000000\n",
      "25%       154.000000\n",
      "50%       187.000000\n",
      "75%       218.000000\n",
      "max       830.000000\n",
      "Name: description, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the length of news article descriptions in both train and test sets\n",
    "print(train_df['description'].apply(len).describe())\n",
    "print(test_df['description'].apply(len).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15436891-fad5-4872-8d38-37be6fe3e567",
   "metadata": {},
   "source": [
    "### Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "327eaa8b-194d-4491-bb4e-95df7bd6ad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Valentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Valentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Valentin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Create the Preprocessing class\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class BasicPreProcessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, stem=False, lem=False):\n",
    "        self.stem = stem\n",
    "        self.lem = lem\n",
    "        self.stopwords = stopwords.words('english')\n",
    "        self.stopwords.extend(['com', 'lt', 'gt', 'quot']) # extend stopwords with words specific to this dataset\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.apply(self._preprocess)\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        text = re.sub(r'target=\\S+|HREF=\\S+', '', text) # remove HREF & Target attributes for URLs\n",
    "        token_text = word_tokenize(text) # break down text into individual tokens\n",
    "        normd_text = [token.lower() for token in token_text if token.isalpha()] # convert to lower-case and remove digits & special characters\n",
    "        swr_text = [token for token in normd_text if token not in self.stopwords] # remove stopwords to keep attention on words that are meaningful\n",
    "\n",
    "        if self.stem: # if stemming is set to True\n",
    "            stemmer = SnowballStemmer(\"english\") # instantiate a Snowball Stemmer\n",
    "            stemmed_text = [stemmer.stem(word) for word in swr_text]\n",
    "            return ' '.join(stemmed_text) # return the processed text\n",
    "\n",
    "        if self.lem: # if lemmatization is set to True\n",
    "            lem = WordNetLemmatizer() # Instantiate a Word Net Lemmatizer\n",
    "            lemmatized_text = [lem.lemmatize(word) for word in swr_text]\n",
    "            return ' '.join(lemmatized_text) # return the processed text\n",
    "            \n",
    "        return ' '.join(swr_text) # return the processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d29117-3402-4e26-ad57-6bf2e68334aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Pre-Processing: chicago reuters cosmetics manufacturer estee lauder tuesday posted bigger quarterly profit bolstered sales europe improving retail market\n",
      "Lem Pre-Processing: chicago reuters cosmetic manufacturer estee lauder tuesday posted bigger quarterly profit bolstered sale europe improving retail market\n",
      "Stem Pre-Processing: chicago reuter cosmet manufactur este lauder tuesday post bigger quarter profit bolster sale europ improv retail market\n"
     ]
    }
   ],
   "source": [
    "# Use a sample description from the dataset to test the pre-processor\n",
    "sample_text = ' CHICAGO (Reuters) - Cosmetics manufacturer Estee Lauder  Cos. Inc. &lt;A HREF=\"http://www.investor.reuters.com/FullQuote.aspx?ticker=EL.N target=/stocks/quickinfo/fullquote\"&gt;EL.N&lt;/A&gt; on Tuesday posted a bigger quarterly profit,  bolstered by sales in Europe and an improving U.S. retail  market.'\n",
    "\n",
    "basic_pre_processor = BasicPreProcessor()\n",
    "lem_pre_processor = BasicPreProcessor(lem=True)\n",
    "stem_pre_processor = BasicPreProcessor(stem=True)\n",
    "\n",
    "print(f\"Basic Pre-Processing: {basic_pre_processor._preprocess(sample_text)}\")\n",
    "print(f\"Lem Pre-Processing: {lem_pre_processor._preprocess(sample_text)}\")\n",
    "print(f\"Stem Pre-Processing: {stem_pre_processor._preprocess(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9233ddc-b989-4069-9c35-07c7133ed82d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>wall bears claw back black reuters reuters wal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>carlyle looks toward commercial aerospace reut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>oil economy cloud stocks outlook reuters reute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>iraq halts oil exports main southern pipeline ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>oil prices soar record posing new menace us ec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                              title  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1  Reuters - Private investment firm Carlyle Grou...   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3  Reuters - Authorities have halted oil export\\f...   \n",
       "4  AFP - Tearaway world oil prices, toppling reco...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  wall bears claw back black reuters reuters wal...  \n",
       "1  carlyle looks toward commercial aerospace reut...  \n",
       "2  oil economy cloud stocks outlook reuters reute...  \n",
       "3  iraq halts oil exports main southern pipeline ...  \n",
       "4  oil prices soar record posing new menace us ec...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-process the texts (title & description) in the training data and save it in add it in a new column\n",
    "train_df['processed_text'] = basic_pre_processor.fit_transform(train_df['title'] + '. ' + train_df['description'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1a39d07-e63a-40af-960f-1af39fed7df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    120000.000000\n",
       "mean        168.528517\n",
       "std          44.700766\n",
       "min           8.000000\n",
       "25%         140.000000\n",
       "50%         166.000000\n",
       "75%         193.000000\n",
       "max         697.000000\n",
       "Name: processed_text, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['processed_text'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5dd912d-0b2f-4238-b0dd-79a70bb02dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall...</td>\n",
       "      <td>fears n pension talks unions representing work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>The Race is On: Second Private Team Sets Launc...</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
       "      <td>race second private team sets launch date huma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry research...</td>\n",
       "      <td>company wins grant study peptides ap ap compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
       "      <td>prediction unit helps forecast wildfires ap ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
       "      <td>calif aims limit smog ap ap southern californi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                              title  \\\n",
       "0      3                  Fears for T N pension after talks   \n",
       "1      4  The Race is On: Second Private Team Sets Launc...   \n",
       "2      4      Ky. Company Wins Grant to Study Peptides (AP)   \n",
       "3      4      Prediction Unit Helps Forecast Wildfires (AP)   \n",
       "4      4        Calif. Aims to Limit Farm-Related Smog (AP)   \n",
       "\n",
       "                                         description  \\\n",
       "0  Unions representing workers at Turner   Newall...   \n",
       "1  SPACE.com - TORONTO, Canada -- A second\\team o...   \n",
       "2  AP - A company founded by a chemistry research...   \n",
       "3  AP - It's barely dawn when Mike Fitzpatrick st...   \n",
       "4  AP - Southern California's smog-fighting agenc...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  fears n pension talks unions representing work...  \n",
       "1  race second private team sets launch date huma...  \n",
       "2  company wins grant study peptides ap ap compan...  \n",
       "3  prediction unit helps forecast wildfires ap ap...  \n",
       "4  calif aims limit smog ap ap southern californi...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-process the texts (title & description) in the test data and save it in add it in a new column\n",
    "test_df['processed_text'] = basic_pre_processor.fit_transform(test_df['title'] + '. ' + test_df['description'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc2aca2-1bf8-40bd-a5ff-8ae116cd5b8f",
   "metadata": {},
   "source": [
    "## Section 2 - Representation Learning\n",
    "\n",
    "(Write more about the 2 approaches taken for learning representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3e5b8-6e22-4d94-ba0e-b2797b660a40",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "087f2fb7-69d5-43cd-ac65-6f85d974fded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.56908003 0.        ]]\n",
      "(120000, 400)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.30646035 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "(7600, 400)\n"
     ]
    }
   ],
   "source": [
    "# First representation - TF-IDF\n",
    "tfidf_text_representation = Pipeline(\n",
    "    [\n",
    "        ('count_vectorizer', CountVectorizer(max_features=400)),\n",
    "        ('tfidf_transformer', TfidfTransformer())\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfidf_train_repr = tfidf_text_representation.fit_transform(train_df['processed_text']).toarray()\n",
    "\n",
    "print(tfidf_train_repr)\n",
    "print(tfidf_train_repr.shape)\n",
    "\n",
    "tfidf_test_repr = tfidf_text_representation.fit_transform(test_df['processed_text']).toarray()\n",
    "\n",
    "print(tfidf_test_repr)\n",
    "print(tfidf_test_repr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fce369-b041-422f-baad-12ad45a4ba97",
   "metadata": {},
   "source": [
    "### Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b03587fc-ae2a-4322-84b9-264b680a8c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "2    10000\n",
      "4    10000\n",
      "1    10000\n",
      "3     9999\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings on a reduced training data\n",
    "target_size = 40000\n",
    "test_size = 1 - (target_size / len(train_df))\n",
    "train_df_reduced, _ = train_test_split(train_df, test_size=test_size, stratify=train_df['class'])\n",
    "\n",
    "print(train_df_reduced['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b1ae3f0-185a-43bb-83ac-15a0053ba34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59837\n"
     ]
    }
   ],
   "source": [
    "# Second representation - Glove\n",
    "\n",
    "texts = train_df['processed_text']\n",
    "target = train_df['class']\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(texts)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1 # calculate the length of the total vocab in the training set\n",
    "print(vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "60572213-5f11-4eff-b8cc-bdda9b01d812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59837, 50)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glove embeddings - use the pre-trained glove embeddings (with dimensionality of 50) to map words to their corresponding vector\n",
    "# representations\n",
    "embeddings_dictionary = dict()\n",
    "embedding_dim = 50\n",
    "\n",
    "with open('./glove_pre_trained/glove.6B.50d.txt', encoding='utf-8') as fp:\n",
    "    for line in fp.readlines():\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a9c94-1fce-4c37-9491-4d4ffeaa8bc4",
   "metadata": {},
   "source": [
    "## Section 3 - Baseline Algorithms\n",
    "\n",
    "(Write more about the choosen algorithms (kNN & Bidirectional LSTM))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69e1499-659f-4410-9982-ec5775df0e2f",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (kNN) with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5de999e5-8d47-4248-9149-d87f2af9b1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.710625, 0.736625, 0.7390833333333333, 0.7417916666666666, 0.7152916666666667]\n"
     ]
    }
   ],
   "source": [
    "knn_tfidf_cls = KNeighborsClassifier(n_neighbors=5)\n",
    "accuracy_scores = []\n",
    "\n",
    "x = tfidf_train_repr\n",
    "y = train_df['class']\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "for train, test in kf.split(x,y):\n",
    "\n",
    "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
    "\n",
    "  knn_tfidf_cls.fit(x_train, y_train)\n",
    "  predictions = knn_tfidf_cls.predict(x_test)\n",
    "  acc = accuracy_score(predictions, y_test)\n",
    "  accuracy_scores.append(acc)\n",
    "\n",
    "print(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e0a11aa9-e642-4b3b-983c-68ee54d1bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the kNN classifier on the full training set then make prediction on the test set\n",
    "knn_pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer(max_features=400)),\n",
    "    ('tfidf_transformer', TfidfTransformer()),  # Transform counts to TF-IDF\n",
    "    ('knn_classifier', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "#knn_pipeline.fit(train_df['processed_text'], train_df['class'])\n",
    "#preds = knn_pipeline.predict(test_df['processed_text'])\n",
    "#print(accuracy_score(preds, test_df['class']))\n",
    "#print(f1_score(test_df['class'], preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8414d04-2f1c-49f0-b958-531e134b5eea",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM with Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13cb5ecc-fa46-47f3-a669-ac45d2a2e0c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bush visits canada tour us president george bush visiting ottawa today first stop tour takes europe early next year first official visit canada since becoming president four years ago skipped canada favour visiting mexico start first term office cancelled state visit canada last year canadians got vocal opposition iraq conflict cbc news reports purpose visit may smooth relations damaged war iraq trade disputes canadian beef softwood lumber bush also expected ask martin stronger longer commitment peacekeeping afghanistan help organizing elections rebuilding political institutions iraq cbc news also speculates bush may offer definitive timetable reopening border canada cattle\n"
     ]
    }
   ],
   "source": [
    "longest_train_text = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "print(longest_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64cff2ac-b64b-4796-b746-d4c5f659ad7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "len_longest_text = len(word_tokenize(longest_train_text))\n",
    "print(len_longest_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f472a5e-be00-4643-a2cc-5e1857e62f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000, 97)\n",
      "(7600, 97)\n"
     ]
    }
   ],
   "source": [
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "train_padded_sentences = pad_sequences(\n",
    "    embed(texts), \n",
    "    len_longest_text, \n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "print(train_padded_sentences.shape)\n",
    "\n",
    "test_padded_sentences = pad_sequences(\n",
    "    embed(test_df['processed_text']), \n",
    "    len_longest_text, \n",
    "    padding='post'\n",
    ")\n",
    "\n",
    "print(test_padded_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7fcde2ec-91d6-46ab-b50c-fe4d7aab76a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,991,850</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d_3               │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │       \u001b[38;5;34m2,991,850\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m)      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d_3               │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_16 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_17 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)                 │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,991,850</span> (11.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,991,850\u001b[0m (11.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,991,850</span> (11.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,991,850\u001b[0m (11.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bidirectional_lstm_model = keras.Sequential(\n",
    "    [\n",
    "        Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights= [embedding_matrix]),\n",
    "        keras.layers.Bidirectional(LSTM(len_longest_text, return_sequences=True)),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.25),\n",
    "        Dense(4, activation='softmax')\n",
    "    ]\n",
    ")\n",
    "\n",
    "bidirectional_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1b0be6-aaea-4384-84b3-d7790fa8ec35",
   "metadata": {},
   "source": [
    "## Section 4 - Evaluation & Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf1919-eec1-4e15-96f9-35a381375b2c",
   "metadata": {},
   "source": [
    "### Evaluation of the kNN classifier with TF-IDF text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5b2676fe-1a70-42a3-b905-8089abcd94fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation mean accuracy score: 0.73\n"
     ]
    }
   ],
   "source": [
    "# KNeighborsClassifier cross-validation using the training data\n",
    "accuracy_scores = []\n",
    "\n",
    "x = train_df['processed_text']\n",
    "y = train_df['class']\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "for train, test in kf.split(x,y):\n",
    "\n",
    "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]\n",
    "\n",
    "  knn_pipeline.fit(x_train, y_train)\n",
    "  predictions = knn_pipeline.predict(x_test)\n",
    "  acc = accuracy_score(predictions, y_test)\n",
    "  accuracy_scores.append(acc)\n",
    "\n",
    "# print(accuracy_scores)\n",
    "print(f'Cross-validation mean accuracy score: {np.mean(accuracy_scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ed38aaa8-7977-4776-93bf-3386e2d8f05c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.82      0.78      1900\n",
      "           2       0.77      0.85      0.81      1900\n",
      "           3       0.79      0.68      0.73      1900\n",
      "           4       0.77      0.73      0.75      1900\n",
      "\n",
      "    accuracy                           0.77      7600\n",
      "   macro avg       0.77      0.77      0.77      7600\n",
      "weighted avg       0.77      0.77      0.77      7600\n",
      "\n",
      "[[1550  168   90   92]\n",
      " [ 152 1622   47   79]\n",
      " [ 197  162 1295  246]\n",
      " [ 169  153  199 1379]]\n"
     ]
    }
   ],
   "source": [
    "# KNeighborsClassifier trained on full train data and evaluated on test data\n",
    "\n",
    "X_train = train_df['processed_text']\n",
    "y_train = train_df['class']\n",
    "\n",
    "X_test = test_df['processed_text']\n",
    "y_test = test_df['class']\n",
    "\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "preds = knn_pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d03af-9460-4cea-8939-fdd752c3e06b",
   "metadata": {},
   "source": [
    "### Evaluation of the Bidirectional LSTM model with Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "348caef6-078b-4f87-9c80-84ddbea1c18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 59ms/step - accuracy: 0.9632 - loss: 0.1145 - val_accuracy: 0.9234 - val_loss: 0.3462\n",
      "Epoch 2/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 58ms/step - accuracy: 0.9707 - loss: 0.0888 - val_accuracy: 0.9214 - val_loss: 0.4021\n",
      "Epoch 3/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 59ms/step - accuracy: 0.9768 - loss: 0.0707 - val_accuracy: 0.9137 - val_loss: 0.4686\n",
      "Epoch 3: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d6980699d0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train the bidirectional_lstm model\n",
    "\n",
    "# Define callback for early stopping\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=2, # number of epochs with no improvement\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "y_train = train_df['class'].apply(lambda x: x - 1).values\n",
    "y_test = test_df['class'].apply(lambda x: x - 1).values\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "bidirectional_lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bidirectional_lstm_model.fit(train_padded_sentences, y_train, batch_size=batch_size, epochs=epochs, validation_data=(test_padded_sentences, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "810ffa05-9b86-47e0-bba5-1dc2f36d21d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step\n",
      "Bidirectional LSTM Accuracy: 0.9136842105263158\n"
     ]
    }
   ],
   "source": [
    "# Make prediction on the test set using the bidirectional LSTM model\n",
    "predictions = [np.argmax(i) for i in bidirectional_lstm_model.predict(test_padded_sentences)]\n",
    "# print(predictions[:5])\n",
    "\n",
    "bidirectional_lstm_model_acc = accuracy_score(predictions, y_test)\n",
    "# bidirectional_lstm_model_f1 = f1_score(y_test, predictions, average='micro')\n",
    "\n",
    "print(f'Bidirectional LSTM Accuracy: {bidirectional_lstm_model_acc}')\n",
    "# print(f'Bidirectional LSTM F1: {bidirectional_lstm_model_f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494252f-6e2e-4c69-9273-65c348e4cd73",
   "metadata": {},
   "source": [
    "## Section 5 - Paper Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961da0e-76c7-4b0e-9719-d262e32be757",
   "metadata": {},
   "source": [
    "## Section 6 - Lbl2Vec Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a27d86-210f-4af1-880f-12f9cc0be17f",
   "metadata": {},
   "source": [
    "### Data Preparation for Lbl2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b011ee48-4feb-4bb9-a9b3-72b90ed280e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_index</th>\n",
       "      <th>class_name</th>\n",
       "      <th>keywords</th>\n",
       "      <th>number_of_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "      <td>[election, state, president, police, politics,...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "      <td>[olympic, football, sport, league, baseball, r...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>[company, market, oil, consumers, exchange, bu...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Science/Technology</td>\n",
       "      <td>[laboratory, computers, science, technology, w...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class_index          class_name  \\\n",
       "0            1               World   \n",
       "1            2              Sports   \n",
       "2            3            Business   \n",
       "3            4  Science/Technology   \n",
       "\n",
       "                                            keywords  number_of_keywords  \n",
       "0  [election, state, president, police, politics,...                  11  \n",
       "1  [olympic, football, sport, league, baseball, r...                  32  \n",
       "2  [company, market, oil, consumers, exchange, bu...                  10  \n",
       "3  [laboratory, computers, science, technology, w...                  18  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load labels with keywords\n",
    "labels = pd.read_csv('ag_news_data/labels.csv',sep=';')\n",
    "\n",
    "# split keywords by separator and save them as array\n",
    "labels['keywords'] = labels['keywords'].apply(lambda x: x.split(' '))\n",
    "\n",
    "# convert description keywords to lowercase\n",
    "labels['keywords'] = labels['keywords'].apply(lambda description_keywords: [keyword.lower() for keyword in description_keywords])\n",
    "\n",
    "# get number of keywords for each class\n",
    "labels['number_of_keywords'] = labels['keywords'].apply(lambda row: len(row))\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fa26df2e-a67f-4bbf-b0e9-0fa1de0498de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>data_set_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>wall bears claw back black reuters reuters wal...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>carlyle looks toward commercial aerospace reut...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>oil economy cloud stocks outlook reuters reute...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>iraq halts oil exports main southern pipeline ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>oil prices soar record posing new menace us ec...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127595</th>\n",
       "      <td>1</td>\n",
       "      <td>Around the world</td>\n",
       "      <td>Ukrainian presidential candidate Viktor Yushch...</td>\n",
       "      <td>around world ukrainian presidential candidate ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127596</th>\n",
       "      <td>2</td>\n",
       "      <td>Void is filled with Clement</td>\n",
       "      <td>With the supply of attractive pitching options...</td>\n",
       "      <td>void filled clement supply attractive pitching...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127597</th>\n",
       "      <td>2</td>\n",
       "      <td>Martinez leaves bitter</td>\n",
       "      <td>Like Roger Clemens did almost exactly eight ye...</td>\n",
       "      <td>martinez leaves bitter like roger clemens almo...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127598</th>\n",
       "      <td>3</td>\n",
       "      <td>5 of arthritis patients in Singapore take Bext...</td>\n",
       "      <td>SINGAPORE : Doctors in the United States have ...</td>\n",
       "      <td>arthritis patients singapore take bextra celeb...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127599</th>\n",
       "      <td>3</td>\n",
       "      <td>EBay gets into rentals</td>\n",
       "      <td>EBay plans to buy the apartment and home renta...</td>\n",
       "      <td>ebay gets rentals ebay plans buy apartment hom...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127600 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class                                              title  \\\n",
       "0           3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1           3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2           3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3           3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4           3  Oil prices soar to all-time record, posing new...   \n",
       "...       ...                                                ...   \n",
       "127595      1                                   Around the world   \n",
       "127596      2                        Void is filled with Clement   \n",
       "127597      2                             Martinez leaves bitter   \n",
       "127598      3  5 of arthritis patients in Singapore take Bext...   \n",
       "127599      3                             EBay gets into rentals   \n",
       "\n",
       "                                              description  \\\n",
       "0       Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1       Reuters - Private investment firm Carlyle Grou...   \n",
       "2       Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3       Reuters - Authorities have halted oil export\\f...   \n",
       "4       AFP - Tearaway world oil prices, toppling reco...   \n",
       "...                                                   ...   \n",
       "127595  Ukrainian presidential candidate Viktor Yushch...   \n",
       "127596  With the supply of attractive pitching options...   \n",
       "127597  Like Roger Clemens did almost exactly eight ye...   \n",
       "127598  SINGAPORE : Doctors in the United States have ...   \n",
       "127599  EBay plans to buy the apartment and home renta...   \n",
       "\n",
       "                                           processed_text data_set_type  \n",
       "0       wall bears claw back black reuters reuters wal...         train  \n",
       "1       carlyle looks toward commercial aerospace reut...         train  \n",
       "2       oil economy cloud stocks outlook reuters reute...         train  \n",
       "3       iraq halts oil exports main southern pipeline ...         train  \n",
       "4       oil prices soar record posing new menace us ec...         train  \n",
       "...                                                   ...           ...  \n",
       "127595  around world ukrainian presidential candidate ...          test  \n",
       "127596  void filled clement supply attractive pitching...          test  \n",
       "127597  martinez leaves bitter like roger clemens almo...          test  \n",
       "127598  arthritis patients singapore take bextra celeb...          test  \n",
       "127599  ebay gets rentals ebay plans buy apartment hom...          test  \n",
       "\n",
       "[127600 rows x 5 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['data_set_type'] = 'train'\n",
    "test_df['data_set_type'] = 'test'\n",
    "\n",
    "full_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "92d4404f-4d81-424a-8120-07a4f7bf6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag pre-processed documents using TaggedDocument\n",
    "full_df['tagged_docs'] = full_df.apply(lambda row: TaggedDocument(row['processed_text'], [str(row.name)]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c50f48ed-2cea-4573-8c14-3e3605c8e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'doc_key' column\n",
    "full_df['doc_key'] = full_df.index.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9d2a8a44-a9d7-431c-90db-0d6c17f6185d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>data_set_type</th>\n",
       "      <th>tagged_docs</th>\n",
       "      <th>doc_key</th>\n",
       "      <th>class_index</th>\n",
       "      <th>class_name</th>\n",
       "      <th>number_of_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>wall bears claw back black reuters reuters wal...</td>\n",
       "      <td>train</td>\n",
       "      <td>(wall bears claw back black reuters reuters wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>carlyle looks toward commercial aerospace reut...</td>\n",
       "      <td>train</td>\n",
       "      <td>(carlyle looks toward commercial aerospace reu...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>oil economy cloud stocks outlook reuters reute...</td>\n",
       "      <td>train</td>\n",
       "      <td>(oil economy cloud stocks outlook reuters reut...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>iraq halts oil exports main southern pipeline ...</td>\n",
       "      <td>train</td>\n",
       "      <td>(iraq halts oil exports main southern pipeline...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>oil prices soar record posing new menace us ec...</td>\n",
       "      <td>train</td>\n",
       "      <td>(oil prices soar record posing new menace us e...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Business</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...   \n",
       "1  Reuters - Private investment firm Carlyle Grou...   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...   \n",
       "3  Reuters - Authorities have halted oil export\\f...   \n",
       "4  AFP - Tearaway world oil prices, toppling reco...   \n",
       "\n",
       "                                      processed_text data_set_type  \\\n",
       "0  wall bears claw back black reuters reuters wal...         train   \n",
       "1  carlyle looks toward commercial aerospace reut...         train   \n",
       "2  oil economy cloud stocks outlook reuters reute...         train   \n",
       "3  iraq halts oil exports main southern pipeline ...         train   \n",
       "4  oil prices soar record posing new menace us ec...         train   \n",
       "\n",
       "                                         tagged_docs doc_key  class_index  \\\n",
       "0  (wall bears claw back black reuters reuters wa...       0            3   \n",
       "1  (carlyle looks toward commercial aerospace reu...       1            3   \n",
       "2  (oil economy cloud stocks outlook reuters reut...       2            3   \n",
       "3  (iraq halts oil exports main southern pipeline...       3            3   \n",
       "4  (oil prices soar record posing new menace us e...       4            3   \n",
       "\n",
       "  class_name  number_of_keywords  \n",
       "0   Business                  10  \n",
       "1   Business                  10  \n",
       "2   Business                  10  \n",
       "3   Business                  10  \n",
       "4   Business                  10  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add 'class_name' column\n",
    "full_df = full_df.merge(labels, left_on='class', right_on='class_index', how='left').drop(['class', 'keywords'], axis=1)\n",
    "\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72780723-bfbb-41bc-a165-962a8f9984b7",
   "metadata": {},
   "source": [
    "### Train Lbl2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7212ba1e-5179-44ad-b8a7-283e45b086c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 12:52:05,725 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2024-12-04 12:52:05,725 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2024-12-04 12:53:52,924 - Lbl2Vec - INFO - Train label embeddings\n",
      "2024-12-04 12:53:52,924 - Lbl2Vec - INFO - Train label embeddings\n",
      "2024-12-04 12:53:52,924 - Lbl2Vec - WARNING - The following keywords from the 'keywords_list' are unknown to the Doc2Vec model and therefore not used to train the model: election state president police politics security war nuclear democracy militant kill\n",
      "2024-12-04 12:53:52,924 - Lbl2Vec - WARNING - The following keywords from the 'keywords_list' are unknown to the Doc2Vec model and therefore not used to train the model: election state president police politics security war nuclear democracy militant kill\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot compute mean with no input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Instantiate and train the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m lbl2vec_model \u001b[38;5;241m=\u001b[39m Lbl2Vec(keywords_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m]), tagged_documents\u001b[38;5;241m=\u001b[39mfull_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtagged_docs\u001b[39m\u001b[38;5;124m'\u001b[39m][full_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_set_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], label_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m]), similarity_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.30\u001b[39m, min_num_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mlbl2vec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lbl2vec\\lbl2vec.py:233\u001b[0m, in \u001b[0;36mLbl2Vec.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain label embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# get doc keys and similarity scores of documents that are similar to\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# the description keywords\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_keys\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_similarity_scores\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription_keywords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_similar_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc2vec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_num_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_num_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_num_docs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# validate that documents to calculate label embeddings from are found\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# for all labels\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_keys\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lbl2vec\\lbl2vec.py:234\u001b[0m, in \u001b[0;36mLbl2Vec.fit.<locals>.<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain label embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# get doc keys and similarity scores of documents that are similar to\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# the description keywords\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_keys\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_similarity_scores\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription_keywords\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_similar_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc2vec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_num_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilarity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_num_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_num_docs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# validate that documents to calculate label embeddings from are found\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# for all labels\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_keys\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lbl2vec\\lbl2vec.py:623\u001b[0m, in \u001b[0;36mLbl2Vec._get_similar_documents\u001b[1;34m(self, doc2vec_model, keywords, num_docs, similarity_threshold, min_num_docs)\u001b[0m\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;66;03m# get documents that are similar to all remaining keywords in\u001b[39;00m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;66;03m# the list\u001b[39;00m\n\u001b[0;32m    621\u001b[0m     keyword_vectors \u001b[38;5;241m=\u001b[39m [doc2vec_model\u001b[38;5;241m.\u001b[39mwv[word]\n\u001b[0;32m    622\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m cleaned_keywords_list]\n\u001b[1;32m--> 623\u001b[0m     similar_docs \u001b[38;5;241m=\u001b[39m \u001b[43mdoc2vec_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeyword_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    626\u001b[0m     error\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    627\u001b[0m                      error\u001b[38;5;241m.\u001b[39margs[\n\u001b[0;32m    628\u001b[0m                          \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in trained Doc2Vec model. Either replace the keyword from the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords_list\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter or train a new Doc2Vec model that knows the keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m,) \u001b[38;5;241m+\u001b[39m error\u001b[38;5;241m.\u001b[39margs[\n\u001b[0;32m    629\u001b[0m                                                                                                                                                                                   \u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[0;32m    844\u001b[0m ]\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py:496\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the mean vector for a given list of keys.\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m \n\u001b[0;32m    494\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot compute mean with no input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    498\u001b[0m     weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(weights)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot compute mean with no input"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the model\n",
    "lbl2vec_model = Lbl2Vec(keywords_list=list(labels['keywords']), tagged_documents=full_df['tagged_docs'][full_df['data_set_type']=='train'], label_names=list(labels['class_name']), similarity_threshold=0.30, min_num_docs=100, epochs=10)\n",
    "lbl2vec_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11390f8d-f2a4-4c5a-88dc-e93fef27841a",
   "metadata": {},
   "source": [
    "## Section 7 - Evaluation of Lbl2Vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
